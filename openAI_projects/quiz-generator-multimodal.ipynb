{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14471727,"sourceType":"datasetVersion","datasetId":9243276}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Install required libraries and system tools\n# 'poppler-utils' is required by pdf2image to convert PDF pages into images.\n!apt-get install -y poppler-utils\n!pip install openai pypdf pdf2image\n\nimport os\nimport base64\nimport io\nimport re\nfrom openai import OpenAI\nfrom pypdf import PdfReader\nfrom pdf2image import convert_from_path\n\n# --- Configuration ---\n\n# 1. Standard Vision Model\n# Reliable high-performance model for general visual tasks.\nVISION_MODEL_VERSION = \"gpt-4o\"\n\n# 2. Latest/SOTA Model\n# The most advanced model available (hypothetical version). \n# Used only when the user explicitly requests the highest performance override.\nLATEST_MODEL_VERSION = \"gpt-5.1\"\n\n# 3. Cost-Effective Text Model\n# Optimized for speed and cost. Used for pages that contain only text.\nTEXT_MODEL_VERSION = \"gpt-4o-mini\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T08:36:26.916770Z","iopub.execute_input":"2026-01-12T08:36:26.917417Z","iopub.status.idle":"2026-01-12T08:36:48.363611Z","shell.execute_reply.started":"2026-01-12T08:36:26.917374Z","shell.execute_reply":"2026-01-12T08:36:48.362505Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 95 not upgraded.\nNeed to get 1,469 kB of archives.\nAfter this operation, 697 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.12 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.12 [5,186 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.12 [1,079 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\nFetched 1,469 kB in 1s (2,169 kB/s)    \n(Reading database ... 129073 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.12_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.12) over (22.02.0-2ubuntu0.11) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.12_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.12) over (22.02.0-2ubuntu0.11) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.12_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.12) over (22.02.0-2ubuntu0.11) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.12) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.12) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.12) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.12) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.12) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\nRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\nRequirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\nRequirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (1.17.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.5)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pdf2image) (11.3.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- API Key Setup ---\n# This block handles secure API key retrieval, specifically designed for Kaggle environments.\n# If running locally, ensure 'OPENAI_API_KEY' is set in your environment variables.\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"OPENAI_API_KEY\")\n\n    # Set the environment variable so the OpenAI client can detect it automatically.\n    os.environ[\"OPENAI_API_KEY\"] = secret_value\n    print(\"Successfully retrieved API key from Kaggle Secrets.\")\nexcept ImportError:\n    # This block executes if not running on Kaggle (e.g., local machine).\n    print(\"Kaggle Secrets not found. Relying on local environment variables.\")\nexcept Exception as e:\n    print(f\"An error occurred while loading secrets: {e}\")\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T08:36:48.365374Z","iopub.execute_input":"2026-01-12T08:36:48.365815Z","iopub.status.idle":"2026-01-12T08:36:48.812519Z","shell.execute_reply.started":"2026-01-12T08:36:48.365782Z","shell.execute_reply":"2026-01-12T08:36:48.811467Z"}},"outputs":[{"name":"stdout","text":"Successfully retrieved API key from Kaggle Secrets.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Utility Functions ---\n\ndef encode_image_to_base64(pil_image):\n    \"\"\"\n    Encodes a PIL image to a Base64 string for API transmission.\n    \n    Args:\n        pil_image (PIL.Image): The image object to encode.\n        \n    Returns:\n        str: Base64 encoded string of the image.\n    \"\"\"\n    buffered = io.BytesIO()\n    # Compress as JPEG to reduce token usage and latency.\n    # Quality=70 provides a good balance between clarity and file size.\n    pil_image.save(buffered, format=\"JPEG\", quality=70)\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\ndef has_visual_keywords(text):\n    \"\"\"\n    Scans the extracted text for specific keywords that indicate the presence of \n    visual content such as figures, equations, or diagrams.\n    \n    Args:\n        text (str): The raw text extracted from a PDF page.\n        \n    Returns:\n        bool: True if visual keywords are found, False otherwise.\n    \"\"\"\n    if not text:\n        return False\n        \n    # List of regex patterns to detect references to visual elements.\n    triggers = [\n        r\"Figure \\d\", r\"Fig\\. \\d\",   # Detects 'Figure 1', 'Fig. 2', etc.\n        r\"Table \\d\",  r\"Tab\\. \\d\",   # Detects 'Table 1', 'Tab. 3', etc.\n        r\"Eq\\. \\d\",   r\"Equation\",   # Detects 'Eq. 4', 'Equation 5'\n        \"Graph\", \"Chart\", \"Diagram\", \"Plot\", \"Schematic\", \"Map\"\n    ]\n    \n    combined_pattern = \"|\".join(triggers)\n    # Perform a case-insensitive search for any of the patterns.\n    return bool(re.search(combined_pattern, text, re.IGNORECASE))\n\ndef prepare_pdf_resources(pdf_path, start_page, end_page):\n    \"\"\"\n    Prepares PDF resources by converting pages to images and initializing the text reader.\n    \n    Args:\n        pdf_path (str): Path to the PDF file.\n        start_page (int): The starting page number (1-based index).\n        end_page (int): The ending page number (1-based index).\n        \n    Returns:\n        tuple: (pdf_images_list, pdf_text_reader) or (None, None) on failure.\n    \"\"\"\n    try:\n        # Convert the specified range of pages into images.\n        # Note: pdf2image uses 1-based indexing for 'first_page' and 'last_page'.\n        images = convert_from_path(pdf_path, first_page=start_page, last_page=end_page)\n        \n        # Initialize the standard PDF text reader.\n        reader = PdfReader(pdf_path)\n        \n        return images, reader\n    except Exception as e:\n        print(f\"Error preparing PDF resources: {e}\")\n        return None, None\n\ndef construct_prompt_messages(mode, content_data):\n    \"\"\"\n    Constructs the message payload for the OpenAI API based on the analysis mode.\n    \n    Args:\n        mode (str): 'vision' (for image analysis) or 'text' (for text-only analysis).\n        content_data (dict): Dictionary containing 'text' and optionally 'image' (base64).\n        \n    Returns:\n        list: A list of message dictionaries formatted for the OpenAI API.\n    \"\"\"\n    if mode == \"vision\":\n        base64_img = content_data['image']\n        return [\n            {   \n                \"role\": \"system\", \n                \"content\": \"You are a quiz generator. Analyze the provided page image (containing text, diagrams, or equations) and generate a high-quality multiple-choice question.\"\n            },\n            {   \n                \"role\": \"user\", \n                \"content\": [\n                    {   \n                        \"type\": \"text\", \n                        \"text\": \"Based on this page image, generate 1 multiple-choice question. Priority: Focus on interpreting diagrams, charts, or equations if they exist. Format:\\nQuestion:\\nOptions:\\n[Correct Answer]:\"\n                    },\n                    {   \n                        \"type\": \"image_url\", \n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\", \"detail\": \"high\"}\n                    }\n                ]\n            }\n        ]\n    else: # text mode\n        text_context = content_data['text']\n        return [\n            {   \n                \"role\": \"system\", \n                \"content\": \"You are a quiz generator based on text context.\"\n            },\n            {   \n                \"role\": \"user\", \n                \"content\": f\"Context:\\n\\\"\\\"\\\"{text_context}\\\"\\\"\\\"\\n\\nGenerate 1 multiple-choice question based on this text. Format:\\nQuestion:\\nOptions:\\n[Correct Answer]:\"\n            }\n        ]\n\ndef call_llm_api(messages, model_name):\n    \"\"\"\n    Calls the OpenAI API using the specified model version and returns the generated content.\n    \n    Args:\n        messages (list): The list of prompt messages.\n        model_name (str): The specific model version to use (e.g., 'gpt-4o', 'gpt-4o-mini').\n        \n    Returns:\n        str: The content generated by the LLM, or None if an error occurs.\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            max_tokens=500, # Set a safe limit to prevent excessive token usage.\n            temperature=0.7\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        print(f\"API Call Error (Model: {model_name}): {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T08:37:41.487171Z","iopub.execute_input":"2026-01-12T08:37:41.488131Z","iopub.status.idle":"2026-01-12T08:37:41.500252Z","shell.execute_reply.started":"2026-01-12T08:37:41.488095Z","shell.execute_reply":"2026-01-12T08:37:41.499305Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- Main Controller Function ---\n\ndef generate_quiz_hybrid(pdf_path, start_page=1, end_page=1, use_latest_model=False):\n    \"\"\"\n    Main controller function for Hybrid (Text/Vision) Quiz Generation with Manual Override.\n    \n    This function iterates through the specified pages of a PDF, decides whether to use \n    Vision (Image) analysis or Text analysis based on content keywords, and selects \n    the appropriate LLM model to optimize for both cost and performance.\n    \n    Args:\n        pdf_path (str): Path to the target PDF file.\n        start_page (int): Page number to start processing (1-based).\n        end_page (int): Page number to stop processing (1-based).\n        use_latest_model (bool): If True, overrides the standard vision model with the \n                                 LATEST_MODEL_VERSION (SOTA) for pages with visual content.\n    \n    Returns:\n        list: A list of generated quiz questions strings.\n    \"\"\"\n    print(f\"Starting Quiz Generation for {pdf_path} (Pages {start_page}-{end_page})...\")\n    \n    # 1. Prepare Resources (Images and Text Reader)\n    pdf_images, pdf_reader = prepare_pdf_resources(pdf_path, start_page, end_page)\n    \n    if not pdf_images or not pdf_reader:\n        print(\"Failed to load PDF resources. Aborting operation.\")\n        return []\n\n    all_questions = []\n\n    # 2. Iterate through each page in the specified range\n    # 'pdf_images' is a list where index 0 corresponds to 'start_page'.\n    for i, image in enumerate(pdf_images):\n        current_page_num = start_page + i\n        \n        try:\n            # Extract text from the page. pypdf uses 0-based indexing.\n            raw_text = pdf_reader.pages[current_page_num - 1].extract_text()\n        except Exception as e:\n            print(f\"Warning: Skipping page {current_page_num} due to text extraction error: {e}\")\n            continue\n            \n        print(f\"\\n--- Processing Page {current_page_num} ---\")\n        \n        # 3. Decision Logic: Determine Mode (Vision vs. Text) and Select Model\n        # Check if the text contains keywords implying visual content (e.g., \"Figure 1\").\n        if has_visual_keywords(raw_text):\n            print(f\">> [Mode: Vision] Visual keywords detected.\")\n            \n            # [Manual Toggle Check] Did the user explicitly request the SOTA model?\n            if use_latest_model:\n                selected_model = LATEST_MODEL_VERSION\n                print(f\"   -> [Override] Using LATEST Model ({selected_model}) as requested.\")\n            else:\n                selected_model = VISION_MODEL_VERSION\n                print(f\"   -> Using Standard Vision Model ({selected_model}).\")\n            \n            mode = \"vision\"\n        else:\n            # For pages with only text, we stick to the cost-effective model to avoid unnecessary expense.\n            # (Note: You could also add logic here to use LATEST model for text if needed)\n            print(f\">> [Mode: Text] Text-only content detected.\")\n            print(f\"   -> Using Cost-Effective Model ({TEXT_MODEL_VERSION}).\")\n            mode = \"text\"\n            selected_model = TEXT_MODEL_VERSION\n            \n        # 4. Prepare Data Payload and Prompt\n        content_data = {'text': raw_text}\n        if mode == \"vision\":\n            # Only encode image if we are in vision mode to save processing time.\n            content_data['image'] = encode_image_to_base64(image)\n            \n        messages = construct_prompt_messages(mode, content_data)\n        \n        # 5. Execute API Call with the Selected Model\n        # We pass the 'selected_model' determined in step 3.\n        generated_question = call_llm_api(messages, model_name=selected_model)\n        \n        if generated_question:\n            print(f\"[Generated Question Preview]: {generated_question[:100]}...\")\n            all_questions.append(generated_question)\n        else:\n            print(\"Failed to generate question for this page.\")\n\n    return all_questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T08:37:46.917830Z","iopub.execute_input":"2026-01-12T08:37:46.918767Z","iopub.status.idle":"2026-01-12T08:37:46.927922Z","shell.execute_reply.started":"2026-01-12T08:37:46.918733Z","shell.execute_reply":"2026-01-12T08:37:46.927098Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- Execution Example ---\n\n# Define the path to your PDF file (Adjust path as needed)\nTARGET_PDF = \"/kaggle/input/qa-test-pdf/2_.pdf\" # Example path for Kaggle\n# TARGET_PDF = \"your_document.pdf\" \n\nif os.path.exists(TARGET_PDF):\n    \n    # Case A: Standard execution (Cost-effective + Standard Vision)\n    print(\"=== Running Standard Mode ===\")\n    # Process page 3 only\n    pg_st_eff = 3\n    pg_en_eff = 3\n    questions_standard = generate_quiz_hybrid(TARGET_PDF, start_page=pg_st_eff, end_page=pg_en_eff, use_latest_model=False)\n\n    \n    # Case B: High-performance execution (Forces LATEST model for visual pages)\n    # Use this for critical sections containing complex diagrams or when standard vision fails.\n    print(\"\\n=== Running High-Performance Mode ===\")\n    # Process page 4 only\n    pg_st_maxP = 3\n    pg_en_maxP = 3\n    questions_latest = generate_quiz_hybrid(TARGET_PDF, start_page=pg_st_maxP, end_page=pg_en_maxP, use_latest_model=True)\n    \n    # Print Results\n    print(\"\\n=== Final Results ===\")\n    for q in questions_standard + questions_latest:\n        print(q)\n        print(\"-\"*40)\nelse:\n    print(f\"File not found: {TARGET_PDF}. Please upload a PDF to test.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T08:39:19.389388Z","iopub.execute_input":"2026-01-12T08:39:19.390366Z","iopub.status.idle":"2026-01-12T08:39:24.809236Z","shell.execute_reply.started":"2026-01-12T08:39:19.390330Z","shell.execute_reply":"2026-01-12T08:39:24.808103Z"}},"outputs":[{"name":"stdout","text":"=== Running Standard Mode ===\nStarting Quiz Generation for /kaggle/input/qa-test-pdf/2_.pdf (Pages 3-3)...\n\n--- Processing Page 3 ---\n>> [Mode: Text] Text-only content detected.\n   -> Using Cost-Effective Model (gpt-4o-mini).\n[Generated Question Preview]: Question: What is the potential consequence of using materials from 메타코드 without permission for comm...\n\n=== Running High-Performance Mode ===\nStarting Quiz Generation for /kaggle/input/qa-test-pdf/2_.pdf (Pages 3-3)...\n\n--- Processing Page 3 ---\n>> [Mode: Text] Text-only content detected.\n   -> Using Cost-Effective Model (gpt-4o-mini).\n[Generated Question Preview]: Question: What is the main responsibility of the materials mentioned in the text regarding quiz ques...\n\n=== Final Results ===\nQuestion: What is the potential consequence of using materials from 메타코드 without permission for commercial purposes?  \nOptions:  \nA) No consequences  \nB) A warning only  \nC) Legal action may be taken  \n[Correct Answer]: C) Legal action may be taken\n----------------------------------------\nQuestion: What is the main responsibility of the materials mentioned in the text regarding quiz questions?  \nOptions:  \nA) They can be freely copied and distributed.  \nB) They are exclusively owned by MetaCode.  \nC) They are intended for personal use only.  \nD) They must be submitted for public access.  \n[Correct Answer]: B) They are exclusively owned by MetaCode.\n----------------------------------------\n","output_type":"stream"}],"execution_count":6}]}